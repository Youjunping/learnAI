{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.反向传播的链式法则\n",
    "\n",
    "链式法则是指递归计算梯度的方法,从损失函数(Loss)开始到输入层,将每一层的梯度(偏导)连续相乘,像链条一样连接在一起,被称为链式法则;神经网络经过一次正向传播得到Loss以后,可以将loss值分给前一层计算出每层的梯度(方向和大小),\n",
    "指明权重更新的方向,这样就可以循环迭代,得到Loss值越来越小的前面每一层权重更新,从而完成自动计算.\n",
    "\n",
    "反向传播借助于链式法则,计算复合函数的导数,将输出单元的梯度反向传回输入单元,根据计算出的梯度,更新神经网络的可学习参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.常见的激活函数及其作用\n",
    "\n",
    "在多层神经网络中,上层节点的输出和下一层节点的输入之间做一个函数变换,这个变换函数就是激活函数;\n",
    "\n",
    "常用的两个激活函数\n",
    "1.sigmoid函数\n",
    "输入x全域,映射到值域(0,1);全方位可导,|X|过大,梯度消散(或爆炸),无法更新参数;\n",
    "x 一般要做归一化处理,变换到(-1,1)区间\n",
    "\n",
    "2.Relu:\n",
    "简单好用,神经网络常用的激活函数\n",
    "\n",
    "作用:\n",
    "不使用激活函数的话，神经网络的每层都只是做线性变换，多层叠加后也还是线性变换。因为线性模型的表达能力通常不够，所以这时候就体现出激活函数的作用，激活函数可以引入非线性因素,增强模型的表达能力."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？\n",
    "\n",
    "可能的原因:\n",
    "1.梯度消失\n",
    "2.参数不更新\n",
    "\n",
    "解决:\n",
    "1.用relu激活函数\n",
    "2.fine tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
